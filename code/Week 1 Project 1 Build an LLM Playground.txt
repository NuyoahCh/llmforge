Overview: An introductory project to explore how prompts, tokenization, and decoding settings work in practice, building the foundation for effective use of large language models. 

Learning Objectives:





Tokenization of raw text into discrete tokens



Basics of GPT-2 and Transformer architectures



Loading pre-trained LLMs with Hugging Face



Decoding strategies for text generation



Completion vs. instruction-tuned models



Estimated Time Commitment: This project typically takes around 90 minutes, depending on how much time you spend experimenting.

Instructions: Open the notebook on GitHub using the link above. At the top of the notebook, click the Colab badge to run it in Google Colab (no installation required). To run locally instead, clone the repository, open the notebook in Jupyter or VS Code, and run it in your Python environment. Then follow these steps:





Start at the top of the notebook and run every cell in order. 



When you see a comment such as `YOUR CODE HERE` inside a code cell, add the missing lines where the comment appears.  



Experiment. Change decoding settings one at a time (temperature, `top_k`, `top_p`), try a few different prompts, and inspect how tokenization affects outputs.



When you get stuck:





Post your question in the Q/A space and get guidance from the instructor or your peers.



If you can’t resolve the issue, make a note of where you stopped and continue with the next sections. The complete solution will be reviewed during the weekly Project Deep Dive session.



When you cannot complete the project: 





Skim through the notebook to familiarize yourself with the main ideas.  



Join the Project Deep Dive session for a full walkthrough and explanation.  



If you cannot attend live, watch the session recording to catch up at your own pace. 



Start here (full project notebook and instructions): 

https://github.com/bytebyteai/ai-eng-projects/blob/main/project_1/lm_playground.ipynb