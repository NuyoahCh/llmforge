Title: LLM Foundations

This lecture introduces the foundations of LLMs, from pre-training data pipelines to model architectures and evaluation methods. The session ties these concepts together with an overview of chatbot system design.

Learning Objectives:





Understand data collection, cleaning, and tokenization for LLMs



Learn key architectural concepts (Transformers, GPT, Llama) and text generation methods



Pre-training and post-training approaches, such as SFT and RLHF



Review evaluation metrics, benchmarks, and human assessment



Assets:





Canvas: https://excalidraw.com/#json=Ew3tR0KylGSscldB8QyJc,DKLaQYILdJU7vHNeqXuJKw



Links mentioned in the video:





AI Index: State of AI in 13 Charts: https://hai.stanford.edu/news/ai-index-state-ai-13-charts





Does Anthropic crawl data from the web, and how can site owners block the crawler?: https://support.anthropic.com/en/articles/8896518-does-anthropic-crawl-data-from-the-web-and-how-can-site-owners-block-the-crawler



GPT2 paper: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf



Common Crawl: https://commoncrawl.org/



Tensorflow C4: https://www.tensorflow.org/datasets/catalog/c4



HuggingFace C4: https://huggingface.co/datasets/allenai/c4



Dolma: https://huggingface.co/papers/2402.00159



Dolma paper: https://arxiv.org/pdf/2402.00159



RefinedWeb: https://arxiv.org/abs/2306.01116



FineWeb: https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1



URL filtering blocklist: https://dsi.ut-capitole.fr/blacklists/



BPE visualization: https://process-mining.tistory.com/189



HuggingFace BPE: https://huggingface.co/learn/llm-course/en/chapter6/5



HuggingFace Llama3: https://huggingface.co/docs/transformers/en/model_doc/llama3



Tiktokenizer: http://tiktokenizer.vercel.app/



Tiktoken library: https://github.com/openai/tiktoken





Attention is All You Need: https://arxiv.org/abs/1706.03762



The illustrated Transformer: https://jalammar.github.io/illustrated-transformer/





Llama 3 paper: https://arxiv.org/abs/2407.21783





Generation strategies: https://huggingface.co/docs/transformers/en/generation_strategies



How to generate text: using different decoding methods for language generation with Transformers: https://huggingface.co/blog/how-to-generate



Instruction tuning datasets: https://huggingface.co/collections/mapama247/instruction-tuning-datasets-65ddec58a16a00a4c84e5cf1



Training language models to follow instructions with human feedback: https://arxiv.org/abs/2203.02155



Alpaca: https://huggingface.co/datasets/tatsu-lab/alpaca

